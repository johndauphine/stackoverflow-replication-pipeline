# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Database Platform Notes

**SQL Server 2022** is the recommended source database platform:
- Production-grade stability and performance
- Full compatibility with StackOverflow2010 database
- Works on AMD64/x86_64 architecture (required for SQL Server 2022)
- 4GB RAM allocation recommended for large datasets

**ARM64 (Apple Silicon) Compatibility:**
- SQL Server 2022 does NOT support ARM64/Apple Silicon
- Use AMD64/x86_64 hardware or cloud VMs (AWS, Azure, GCP)
- Alternative: Use PostgreSQL as both source and target on ARM64

**PostgreSQL Target Benefits:**
- Cross-platform compatibility (works on ARM64)
- No licensing costs
- Excellent performance for data warehousing workloads
- Better stability on non-Windows platforms

---

## Network Architecture

**Production-Like Design**: Database containers run on **separate networks** from Airflow, simulating real production environments where databases are external services (AWS RDS, Azure SQL, etc.).

```
┌──────────────────┐
│  Astro Network   │          External Databases
│  ┌────────────┐  │          (Separate Networks)
│  │  Airflow   │  │
│  │ Scheduler  │──┼──► HOST_IP:1433 ──► SQL Server Source
│  └────────────┘  │   (via host network)
│                  │
│                  │──► HOST_IP:1434 ──► SQL Server Target
└──────────────────┘   (via host network)
```

**Platform-Specific Host IP:**
- **Linux** (ChromeOS, Ubuntu, etc.): `172.17.0.1` (Docker bridge gateway)
- **macOS/Windows**: `host.docker.internal` (Docker Desktop special hostname)

**Why This Design?**
- ✓ Simulates production where databases are on separate servers/VMs
- ✓ Tests real TCP/IP network communication (not just Docker bridge)
- ✓ Validates firewall/port-based access control
- ✓ Proves pipelines work with external database services
- ✓ More secure - databases isolated from Airflow infrastructure

---

## Quick Start: Complete Environment Setup

### Step 1: Start Astro
```bash
astro dev start
```

### Step 2: Set Up SQL Server Databases

**SQL Server-to-SQL Server Replication Pipeline:**

```bash
# Start SQL Server 2022 source with StackOverflow2010 database
# Note: The .mdf and .ldf files are in include/stackoverflow/
# Requires AMD64/x86_64 architecture (SQL Server 2022 does not support ARM64)
# Port 1433 exposed to host (NOT on Astro network - production-like design)
docker run -d --name stackoverflow-mssql-source \
  --platform linux/amd64 \
  --memory="4g" \
  -e "ACCEPT_EULA=Y" -e "MSSQL_SA_PASSWORD=StackOverflow123!" \
  -e "MSSQL_PID=Developer" \
  -v "$(pwd)/include/stackoverflow":/var/opt/mssql/backup \
  -p 1433:1433 mcr.microsoft.com/mssql/server:2022-latest

# Start SQL Server 2022 target (4GB RAM for heavy write operations)
# Port 1434 exposed to host (NOT on Astro network - production-like design)
docker run -d --name stackoverflow-mssql-target \
  --platform linux/amd64 \
  --memory="4g" \
  -e "ACCEPT_EULA=Y" -e "MSSQL_SA_PASSWORD=StackOverflow123!" \
  -e "MSSQL_PID=Developer" \
  -p 1434:1433 mcr.microsoft.com/mssql/server:2022-latest
```

**Note:** Databases are **NOT** connected to the Astro network. Airflow accesses them via host network, simulating external database servers.

### Step 3: Attach Source Database

```bash
# Copy database files into source container
docker exec stackoverflow-mssql-source mkdir -p /var/opt/mssql/data
docker cp include/stackoverflow/StackOverflow2010.mdf stackoverflow-mssql-source:/var/opt/mssql/data/
docker cp include/stackoverflow/StackOverflow2010_log.ldf stackoverflow-mssql-source:/var/opt/mssql/data/

# Fix file permissions (must run as root)
docker exec -u root stackoverflow-mssql-source chown mssql:mssql /var/opt/mssql/data/StackOverflow2010.mdf /var/opt/mssql/data/StackOverflow2010_log.ldf
docker exec -u root stackoverflow-mssql-source chmod 660 /var/opt/mssql/data/StackOverflow2010.mdf /var/opt/mssql/data/StackOverflow2010_log.ldf

# Attach the database (note: do NOT use quotes around password in this command)
docker exec stackoverflow-mssql-source /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P StackOverflow123! -C -Q \
  "CREATE DATABASE StackOverflow2010 ON (FILENAME = '/var/opt/mssql/data/StackOverflow2010.mdf'), (FILENAME = '/var/opt/mssql/data/StackOverflow2010_log.ldf') FOR ATTACH"
```

### Step 4: Create Airflow Connections

**IMPORTANT**: Connection host must match your platform. Using the wrong host will cause "Unable to connect" or "Connection refused" errors.

**For Linux (ChromeOS, Ubuntu, etc.):**
```bash
# Source connection (via Docker bridge gateway)
astro dev run connections add stackoverflow_source \
  --conn-type mssql --conn-host 172.17.0.1 --conn-port 1433 \
  --conn-login sa --conn-password "StackOverflow123!" --conn-schema StackOverflow2010

# Target connection (via Docker bridge gateway)
astro dev run connections add stackoverflow_target \
  --conn-type mssql --conn-host 172.17.0.1 --conn-port 1434 \
  --conn-login sa --conn-password "StackOverflow123!" --conn-schema master
```

**For macOS/Windows:**
```bash
# Source connection (via Docker Desktop host gateway)
astro dev run connections add stackoverflow_source \
  --conn-type mssql --conn-host host.docker.internal --conn-port 1433 \
  --conn-login sa --conn-password "StackOverflow123!" --conn-schema StackOverflow2010

# Target connection (via Docker Desktop host gateway)
astro dev run connections add stackoverflow_target \
  --conn-type mssql --conn-host host.docker.internal --conn-port 1434 \
  --conn-login sa --conn-password "StackOverflow123!" --conn-schema master
```

**Verification:**
```bash
# Verify connections are configured correctly
astro dev run connections list | grep stackoverflow

# Test connectivity from Airflow
astro dev run tasks test replicate_stackoverflow_to_target reset_target_schema 2025-11-10
```

### Step 5: Run DAGs

```bash
# Replicate Stack Overflow data from source to target
astro dev run dags unpause replicate_stackoverflow_to_target
astro dev run dags trigger replicate_stackoverflow_to_target
```

### Step 6: Verify

```bash
# Check row counts on target
docker exec stackoverflow-mssql-target /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P "StackOverflow123!" -C -Q \
  "USE stackoverflow_target; SELECT 'Users' AS TableName, COUNT(*) AS RowCount FROM dbo.Users \
   UNION ALL SELECT 'Posts', COUNT(*) FROM dbo.Posts \
   UNION ALL SELECT 'Comments', COUNT(*) FROM dbo.Comments \
   UNION ALL SELECT 'Votes', COUNT(*) FROM dbo.Votes \
   UNION ALL SELECT 'Badges', COUNT(*) FROM dbo.Badges;"
```

Expected row counts (StackOverflow2010 database):
- Users: ~315,000
- Posts: ~1.7 million
- Comments: ~1.3 million
- Votes: ~4.3 million
- Badges: ~190,000

---

## Alternative Setup: SQL Server to PostgreSQL Replication

> **⚠️ CRITICAL: macOS ARM64 (Apple Silicon) Limitation**
>
> **SQL Server 2022 does NOT natively support ARM64.** On M-series Macs:
> - SQL Server runs in **x86_64 emulation** (Rosetta 2)
> - **Performance**: Slower (emulation overhead)
> - **Stability**: Can crash during heavy operations
> - **Best for macOS ARM64**: Run SQL Server source on cloud AMD64 VM (AWS/Azure/GCP)
> - **Alternative**: Use PostgreSQL → PostgreSQL (fully native ARM64)
>
> For local testing on ARM64, expect slower performance and potential crashes with large datasets.

### Step 1: Start Astro
```bash
astro dev start
```

### Step 2: Set Up SQL Server Source and PostgreSQL Target

**SQL Server-to-PostgreSQL Replication Pipeline:**

```bash
# Start SQL Server 2022 source with StackOverflow2010 database
# Note: On ARM64 (Apple Silicon), this runs in x86_64 emulation mode (slower, less stable)
# Requires AMD64/x86_64 architecture OR ARM64 with emulation
# Port 1433 exposed to host (NOT on Astro network - production-like design)
docker run -d --name stackoverflow-mssql-source \
  --platform linux/amd64 \
  --memory="4g" \
  -e "ACCEPT_EULA=Y" -e "MSSQL_SA_PASSWORD=StackOverflow123!" \
  -e "MSSQL_PID=Developer" \
  -v "$(pwd)/include/stackoverflow":/var/opt/mssql/backup \
  -p 1433:1433 mcr.microsoft.com/mssql/server:2022-latest

# Start PostgreSQL 16 target (cross-platform, works on ARM64 and AMD64)
# Port 5433 to avoid conflict with Astro's internal Postgres on 5432
# NOT on Astro network - production-like design
docker run -d --name stackoverflow-postgres-target \
  -e "POSTGRES_PASSWORD=StackOverflow123!" \
  -e "POSTGRES_USER=postgres" \
  -e "POSTGRES_DB=stackoverflow_target" \
  -p 5433:5432 postgres:16
```

**Note:** Databases are **NOT** connected to the Astro network. Airflow accesses them via host network, simulating external database services like AWS RDS or Azure Database for PostgreSQL.

### Step 3: Attach Source Database

```bash
# Copy database files into source container
docker exec stackoverflow-mssql-source mkdir -p /var/opt/mssql/data
docker cp include/stackoverflow/StackOverflow2010.mdf stackoverflow-mssql-source:/var/opt/mssql/data/
docker cp include/stackoverflow/StackOverflow2010_log.ldf stackoverflow-mssql-source:/var/opt/mssql/data/

# Fix file permissions (must run as root)
docker exec -u root stackoverflow-mssql-source chown mssql:mssql /var/opt/mssql/data/StackOverflow2010.mdf /var/opt/mssql/data/StackOverflow2010_log.ldf
docker exec -u root stackoverflow-mssql-source chmod 660 /var/opt/mssql/data/StackOverflow2010.mdf /var/opt/mssql/data/StackOverflow2010_log.ldf

# Attach the database (note: do NOT use quotes around password in this command)
docker exec stackoverflow-mssql-source /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P StackOverflow123! -C -Q \
  "CREATE DATABASE StackOverflow2010 ON (FILENAME = '/var/opt/mssql/data/StackOverflow2010.mdf'), (FILENAME = '/var/opt/mssql/data/StackOverflow2010_log.ldf') FOR ATTACH"
```

### Step 4: Create Airflow Connections

**IMPORTANT**: Connection host must match your platform. Using the wrong host will cause "Unable to connect" or "Connection refused" errors.

**For Linux (ChromeOS, Ubuntu, etc.):**
```bash
# Source connection - SQL Server (via Docker bridge gateway)
astro dev run connections add stackoverflow_source \
  --conn-type mssql --conn-host 172.17.0.1 --conn-port 1433 \
  --conn-login sa --conn-password "StackOverflow123!" --conn-schema StackOverflow2010

# Target connection - PostgreSQL (via Docker bridge gateway)
astro dev run connections add stackoverflow_postgres_target \
  --conn-type postgres --conn-host 172.17.0.1 --conn-port 5433 \
  --conn-login postgres --conn-password "StackOverflow123!" --conn-schema stackoverflow_target
```

**For macOS/Windows:**
```bash
# Source connection - SQL Server (via Docker Desktop host gateway)
astro dev run connections add stackoverflow_source \
  --conn-type mssql --conn-host host.docker.internal --conn-port 1433 \
  --conn-login sa --conn-password "StackOverflow123!" --conn-schema StackOverflow2010

# Target connection - PostgreSQL (via Docker Desktop host gateway)
astro dev run connections add stackoverflow_postgres_target \
  --conn-type postgres --conn-host host.docker.internal --conn-port 5433 \
  --conn-login postgres --conn-password "StackOverflow123!" --conn-schema stackoverflow_target
```

**Verification:**
```bash
# Verify connections are configured correctly
astro dev run connections list | grep stackoverflow

# Test connectivity from Airflow
astro dev run tasks test replicate_stackoverflow_to_postgres reset_target_schema 2025-11-10
```

### Step 5: Run DAGs

```bash
# Replicate Stack Overflow data from SQL Server to PostgreSQL
astro dev run dags unpause replicate_stackoverflow_to_postgres
astro dev run dags trigger replicate_stackoverflow_to_postgres
```

### Step 6: Verify

```bash
# Check row counts on PostgreSQL target
docker exec stackoverflow-postgres-target psql -U postgres -d stackoverflow_target -c \
  "SELECT 'Users' AS tablename, COUNT(*) AS rowcount FROM \"Users\"
   UNION ALL SELECT 'Posts', COUNT(*) FROM \"Posts\"
   UNION ALL SELECT 'Comments', COUNT(*) FROM \"Comments\"
   UNION ALL SELECT 'Votes', COUNT(*) FROM \"Votes\"
   UNION ALL SELECT 'Badges', COUNT(*) FROM \"Badges\"
   ORDER BY tablename;"
```

Expected row counts (StackOverflow2010 database):
- Users: ~315,000
- Posts: ~1.7 million
- Comments: ~1.3 million
- Votes: ~4.3 million
- Badges: ~190,000

**Key Differences from SQL Server Target:**
- Uses PostgreSQL COPY command for faster bulk loading
- Automatic data type conversion (NVARCHAR → VARCHAR, DATETIME → TIMESTAMP, BIT → BOOLEAN)
- GENERATED ALWAYS AS IDENTITY instead of IDENTITY(1,1)
- Case-sensitive table/column names (quoted identifiers)
- No need for SET IDENTITY_INSERT equivalent
- Uses pg8000 driver (pure Python, fork-safe for LocalExecutor)

---

## Project Overview

This is a **Stack Overflow End-to-End Data Replication Pipeline** using Apache Airflow 3 with support for both SQL Server and PostgreSQL targets. It demonstrates production-quality data engineering practices including memory-capped streaming replication, audit logging, and resource management for large-scale datasets.

**Key Components:**
- Source SQL Server database (port 1433) with StackOverflow2010 sample data (Brent Ozar edition)
- Target databases: SQL Server (port 1434) or PostgreSQL (port 5433)
- Apache Airflow DAGs for orchestration
- Audit trail and data validation
- Memory-efficient streaming replication with 128MB buffer
- Cross-database data type mapping and conversion
- **Production-like network architecture**: Databases on separate networks, accessed via host IP (simulates AWS RDS, Azure SQL, etc.)

**Stack Overflow Database:**
- Source: Brent Ozar's StackOverflow2010 database
- Size: 8.4GB (.mdf) + 256MB (.ldf)
- Data: 2008-2010 Stack Overflow posts, users, comments, votes, badges
- License: CC-BY-SA 3.0
- Download: https://downloads.brentozar.com/StackOverflow2010.7z

## Development Commands

### Environment Management
```bash
astro dev start          # Start local Airflow environment
astro dev restart        # Reload after dependency changes (requirements.txt, DAG code)
astro dev stop           # Stop all containers
```

### Testing
```bash
astro dev run pytest tests/dags                                      # Run DAG validation tests
astro dev run dags test replicate_stackoverflow_to_target <date>     # Dry-run SQL Server DAG
astro dev run dags test replicate_stackoverflow_to_postgres <date>   # Dry-run PostgreSQL DAG
```

### DAG Operations
```bash
# SQL Server to SQL Server
astro dev run dags unpause replicate_stackoverflow_to_target   # Enable DAG scheduling
astro dev run dags trigger replicate_stackoverflow_to_target   # Manual trigger

# SQL Server to PostgreSQL
astro dev run dags unpause replicate_stackoverflow_to_postgres
astro dev run dags trigger replicate_stackoverflow_to_postgres

astro dev run connections list                                  # View Airflow connections
```

### Database Inspection

```bash
# Check source database tables
docker exec stackoverflow-mssql-source /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P "StackOverflow123!" -C -Q \
  "USE StackOverflow2010; SELECT name FROM sys.tables ORDER BY name;"

# Check target database tables
docker exec stackoverflow-mssql-target /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P "StackOverflow123!" -C -Q \
  "USE stackoverflow_target; SELECT name FROM sys.tables ORDER BY name;"
```

## Architecture Overview

### Network Design

**Production-Like External Database Access:**

Databases run on **separate Docker networks** from Airflow, accessed via host IP addresses. This simulates real production environments where databases are external services.

```
┌─────────────────────────────────────────┐
│  Docker Host (172.17.0.1)              │
│                                         │
│  ┌─────────────────────┐                │
│  │ Astro Network       │                │
│  │ ┌───────────────┐   │                │
│  │ │ Airflow       │   │  TCP/IP        │  ┌──────────────┐
│  │ │ Scheduler     │───┼───────►:1433──┼──│ SQL Server   │
│  │ └───────────────┘   │    Network     │  │ Source       │
│  │ ┌───────────────┐   │    Stack       │  └──────────────┘
│  │ │ Airflow       │   │                │
│  │ │ Workers       │───┼───────►:5433──┼──┐
│  │ └───────────────┘   │                │  │
│  └─────────────────────┘                │  │ ┌──────────────┐
│                                         │  └─│ PostgreSQL   │
└─────────────────────────────────────────┘    │ Target       │
                                               └──────────────┘
```

**Network Path:**
1. Airflow containers connect to `172.17.0.1:1433` (Linux) or `host.docker.internal:1433` (macOS/Windows)
2. Request goes through host network stack (TCP/IP)
3. Port forwarding maps `host:1433` → `container:1433`
4. Database container receives connection on its exposed port

**Why This Design?**
- ✓ **Realistic testing**: Mimics AWS RDS, Azure SQL Database, Google Cloud SQL
- ✓ **Network isolation**: Databases can't see Airflow infrastructure
- ✓ **Security validation**: Tests firewall rules and port-based access control
- ✓ **Performance testing**: Includes real network stack overhead
- ✓ **Production parity**: Same connection patterns as production deployments

**Alternative (NOT recommended for production-like testing):**
- Shared Docker network: All containers on same bridge
- Uses Docker DNS: `stackoverflow-mssql-source:1433`
- No network isolation or realistic latency
- Simpler but less representative of production

### DAG Structure

**`replicate_stackoverflow_to_target.py`** - SQL Server to SQL Server replication
- Creates target database and schema
- Copies tables in dependency order
- Uses CSV-based bulk loading with batch INSERT
- Memory limit: 128MB in-memory buffer before disk spill
- Logs memory usage and disk spill events
- Aligns identity sequences after copy (DBCC CHECKIDENT)

**`replicate_stackoverflow_to_postgres.py`** - SQL Server to PostgreSQL replication
- Creates target database and schema with cross-database type mapping
- Converts SQL Server data types to PostgreSQL equivalents
- Uses PostgreSQL COPY command for efficient bulk loading
- Memory limit: 128MB in-memory buffer before disk spill
- Handles identity column conversion (IDENTITY → GENERATED ALWAYS AS IDENTITY)
- Resets PostgreSQL sequences after copy using setval()

### Table Dependency Order

```
Users → Badges
     → Posts → PostHistory
             → PostLinks
             → Comments
             → Votes
     → Tags (if present)
     → VoteTypes
```

### Key Configuration
- **Resource Allocation**: Scheduler (2Gi), Webserver (1Gi), Triggerer (512Mi)
- **Parallelism**: 16 parallel tasks, 8 active tasks per DAG, 2 active runs per DAG
- **Memory Streaming**: 128MB in-memory buffer before disk spill

## Code Conventions

### DAG Requirements (Enforced by Tests)
- All DAGs must have tags
- All DAGs must have `retries >= 2` in `default_args`
- No import errors allowed

### Connection IDs
- `stackoverflow_source` - Source SQL Server database (StackOverflow2010)
- `stackoverflow_target` - Target SQL Server database
- `stackoverflow_postgres_target` - Target PostgreSQL database

### Python Style
- Python 3.10+ with type hints
- PEP 8 spacing (4-space indents)
- `snake_case` for DAG IDs and task IDs
- Module-level uppercase constants for shared values

## Technology Stack

- **Apache Airflow 3** on Astro Runtime 3.1-3
- **Microsoft SQL Server 2022** (Developer Edition) for source database
- **PostgreSQL 16** for optional target database
- **Python 3.10+** with pymssql and pg8000 (pure Python drivers)
- **Docker** for database containerization
- **pytest** for DAG validation

## Design Decisions

### Why Copy Database Files Instead of Mounting?

The setup instructions **copy** `.mdf` and `.ldf` files into containers rather than mounting them as volumes:

```bash
# ✓ What we do (copy)
docker cp StackOverflow2010.mdf stackoverflow-mssql-source:/var/opt/mssql/data/

# ✗ What we DON'T do (mount)
# docker run -v "$(pwd)/include/stackoverflow:/var/opt/mssql/data" ...
```

**Reasons:**

1. **File Permission Requirements**
   - SQL Server requires specific ownership (`mssql:mssql`) and permissions (`660`)
   - Mounted volumes inherit host filesystem permissions
   - Container UID/GID (e.g., `mssql` = 10001) may not map to host users
   - Many filesystems (macOS/Windows) don't support Unix ownership in containers

2. **SQL Server I/O Requirements**
   - Needs direct I/O, file locking, and specific fsync semantics
   - Mounted volumes (especially on macOS/Windows) go through VM/network layer
   - Performance degradation: 10-100x slower on mounted volumes
   - Risk of corruption if filesystem doesn't honor fsync properly

3. **Cross-Platform Compatibility**
   - **Copying works consistently** on Linux, macOS, Windows, ChromeOS
   - **Mounting** has platform-specific issues:
     - macOS: osxfs/VirtioFS performance penalties
     - Windows: NTFS → Linux translation issues
     - ChromeOS: Additional LXD container complexity

4. **Database Upgrade Process**
   - SQL Server modifies files in-place during version upgrades
   - Requires full read/write access and ability to extend file size
   - Our test showed: `Converting database from version 655 to 957`
   - Mounted volumes may block or corrupt during upgrades

**When to Use Volumes:**

For **production databases** where data persistence is required:
```bash
docker run -v sqldata:/var/opt/mssql/data mssql/server:2022-latest
```
- Docker-managed volumes use container's native filesystem
- No host permission mapping issues
- Data persists across container recreations

**Trade-offs:**

| Approach | Pros | Cons |
|----------|------|------|
| **Copy** (our choice) | Reliable permissions, cross-platform, better performance | Uses 2x disk space, slower initial setup |
| **Mount** | No duplication, easy host access | Permission issues, platform-specific, potential corruption |

For this **demo/testing scenario** with a read-only source database, copying is the correct choice.

---

## Troubleshooting

**IMPORTANT**: For comprehensive infrastructure troubleshooting (container crashes, authentication issues, connection problems), see [docs/troubleshooting-infrastructure.md](docs/troubleshooting-infrastructure.md).

### Quick Diagnostics

```bash
# Check all containers are running
docker ps --filter "name=stackoverflow"

# Test SQL Server connectivity
docker exec stackoverflow-mssql-source /opt/mssql-tools18/bin/sqlcmd \
  -S localhost -U sa -P StackOverflow123! -C -Q "SELECT 'Source OK' AS Status;"

docker exec stackoverflow-mssql-target /opt/mssql-tools18/bin/sqlcmd \
  -S localhost -U sa -P StackOverflow123! -C -Q "SELECT 'Target OK' AS Status;"

# Verify Airflow connections (macOS/Windows should use host.docker.internal)
astro dev run connections list | grep stackoverflow
```

### Common Issues Summary

| Issue | Symptom | Quick Fix |
|-------|---------|-----------|
| **Container crashes** | Exit code 1, core dumps | Use AMD64 hardware or PostgreSQL |
| **Auth failures** | "Login failed for user 'sa'" | Wait 45+ seconds after container start |
| **Connection refused** | "Unable to connect" | Update connections to use `host.docker.internal` (macOS/Windows) or `172.17.0.1` (Linux) |
| **BCP failures** | "docker: not found" | Use optimized DAG instead |

### Database File Permissions Issue (SQL Server)

If you encounter permission errors when attaching the StackOverflow2010 database:

**Problem:**
```
Msg 3415, Level 16, State 2, Server ..., Line 1
Database 'StackOverflow2010' cannot be upgraded because it is read-only,
has read-only files or the user does not have permissions to modify
some of the files. Make the database or files writeable, and rerun recovery.
```

**Root Cause:**
- Files copied with `docker cp` inherit host filesystem permissions
- SQL Server process runs as `mssql` user inside container
- Copied .mdf/.ldf files may not be owned by `mssql:mssql`
- Insufficient permissions prevent database upgrade/recovery

**Solution:**

Fix ownership and permissions as root user before attaching:

```bash
# Set ownership to mssql user
docker exec -u root stackoverflow-mssql-source chown mssql:mssql \
  /var/opt/mssql/data/StackOverflow2010.mdf \
  /var/opt/mssql/data/StackOverflow2010_log.ldf

# Set read/write permissions (660)
docker exec -u root stackoverflow-mssql-source chmod 660 \
  /var/opt/mssql/data/StackOverflow2010.mdf \
  /var/opt/mssql/data/StackOverflow2010_log.ldf
```

**Why This Works:**
- `chown mssql:mssql` grants ownership to SQL Server process user
- `chmod 660` allows read/write for owner and group, no access for others
- SQL Server can now upgrade database from 2008 format to 2022 format
- Database recovery completes successfully

**Note:** Running `chmod` as non-root will fail with "Operation not permitted"

### SQL Server SA Authentication Issue

If you encounter login failures when running sqlcmd commands:

**Problem:**
```
Sqlcmd: Error: Microsoft ODBC Driver 18 for SQL Server : Login failed for user 'sa'.
```

**Root Cause:**
- Password contains special characters (e.g., `StackOverflow123!`)
- Shell may interpret quotes inconsistently depending on context
- `docker exec` with quoted password fails in certain scenarios

**Solution:**

**For CREATE DATABASE and most queries:** Remove quotes from password parameter:
```bash
# ✓ CORRECT - No quotes
docker exec stackoverflow-mssql-source /opt/mssql-tools18/bin/sqlcmd \
  -S localhost -U sa -P StackOverflow123! -C -Q "SELECT @@VERSION"

# ✗ WRONG - Quoted password fails
docker exec stackoverflow-mssql-source /opt/mssql-tools18/bin/sqlcmd \
  -S localhost -U sa -P "StackOverflow123!" -C -Q "SELECT @@VERSION"
```

**For Airflow connections:** Use quotes to prevent shell expansion:
```bash
astro dev run connections add stackoverflow_source \
  --conn-password "StackOverflow123!"  # Quotes needed here
```

**Why This Works:**
- Without quotes, shell passes password directly to sqlcmd
- sqlcmd receives the exact password string including `!`
- With quotes in `docker exec`, shell may escape or modify the password

**Alternative:** If authentication continues to fail:
1. Wait 15-30 seconds after container starts for SA password initialization
2. Restart container: `docker restart stackoverflow-mssql-source && sleep 30`
3. Use `-d` flag to specify database: `-d StackOverflow2010`

### PostgreSQL DAG with LocalExecutor

If you experience fork deadlocks with the PostgreSQL DAG when using LocalExecutor:

**Problem:** Tasks succeed but are marked as failed with `SIGKILL: -9`
**Cause:** psycopg2 (C extension) creates background threads that break during fork()
**Solution:** Use pg8000 (pure Python driver) instead

Already configured in `requirements.txt`. If you have issues:
1. Ensure `pg8000>=1.29.0` is in requirements.txt
2. Run `astro dev restart` to rebuild with pg8000
3. Connection will automatically use pg8000 when available

**Alternative solutions:**
- Use CeleryExecutor (for production)
- Use KubernetesExecutor (for cloud deployments)
- Use `astro dev run dags test` for development testing

### Empty String to NULL Conversion Issue (PostgreSQL)

If you encounter `NotNullViolation` errors during PostgreSQL replication:

**Problem:**
```
psycopg2.errors.NotNullViolation: null value in column "Body" of relation "Posts" violates not-null constraint
DETAIL: Failing row contains (...Body = null...)
```

**Root Cause:**
- SQL Server allows empty strings ('') in NOT NULL columns
- During CSV export/import, empty strings are converted to NULL values
- PostgreSQL enforces NOT NULL constraints strictly, rejecting NULL values
- Approximately 10 rows in Posts table have empty Body values in StackOverflow2010

**Solution (Implemented in `dags/replicate_stackoverflow_to_postgres.py`):**

Modified schema creation to allow NULL for text/varchar columns even when source schema defines NOT NULL:

```python
# Special case: Allow NULL for text columns even if source says NOT NULL
# to handle empty strings that get converted to NULL during CSV export
if is_nullable == 'NO' and pg_type not in ('TEXT', 'VARCHAR') and not pg_type.startswith('VARCHAR('):
    col_def += " NOT NULL"
else:
    col_def += " NULL"
```

**Why This Works:**
- Empty strings in source become NULL in target (acceptable data loss)
- Preserves data integrity for truly NULL values
- Prevents replication failures on edge cases
- Maintains NOT NULL constraints on non-text columns (integers, dates, etc.)

**Alternative Solutions:**
1. Pre-process source data to replace empty strings with a placeholder (e.g., '[EMPTY]')
2. Use COALESCE() in the SELECT query to replace empty strings with a default value
3. Modify target schema to use CHECK constraints instead of NOT NULL

## Stack Overflow Database Schema

### Main Tables

| Table | Description | Approximate Rows (2010) |
|-------|-------------|------------------------|
| Users | User accounts and profiles | 315,000 |
| Posts | Questions and answers | 1,700,000 |
| Comments | Comments on posts | 1,300,000 |
| Votes | Upvotes/downvotes | 4,300,000 |
| Badges | User achievements | 190,000 |
| PostHistory | Edit history | 2,800,000 |
| PostLinks | Related/duplicate posts | 100,000 |
| Tags | Question categorization | 13,000 |
| VoteTypes | Vote type lookup | ~15 |

### Key Relationships

- `Users.Id` → `Posts.OwnerUserId`, `Comments.UserId`, `Badges.UserId`
- `Posts.Id` → `Comments.PostId`, `Votes.PostId`, `PostHistory.PostId`, `PostLinks.PostId`
- `Posts.ParentId` → `Posts.Id` (answers reference questions)
- `VoteTypes.Id` → `Votes.VoteTypeId`

## Key Files

- `dags/replicate_stackoverflow_to_target.py` - SQL Server to SQL Server replication
- `dags/replicate_stackoverflow_to_postgres.py` - SQL Server to PostgreSQL replication
- `include/stackoverflow/StackOverflow2010.mdf` - Source database file (8.4GB)
- `include/stackoverflow/StackOverflow2010_log.ldf` - Transaction log (256MB)
- `include/stackoverflow/Readme_2010.txt` - Database documentation
- `tests/dags/test_dag_example.py` - DAG validation tests
- `.astro/config.yaml` - Resource allocation settings
- `Dockerfile` - Airflow parallelism configuration
- `CLAUDE.md` - AI assistant guidance and setup instructions

## Verification Commands

### Row Count Verification

```bash
# Comprehensive row count check
docker exec stackoverflow-mssql-target /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P "StackOverflow123!" -C -Q \
  "USE stackoverflow_target;
   SELECT 'Users' AS TableName, COUNT(*) AS RowCount FROM dbo.Users
   UNION ALL SELECT 'Posts', COUNT(*) FROM dbo.Posts
   UNION ALL SELECT 'Comments', COUNT(*) FROM dbo.Comments
   UNION ALL SELECT 'Votes', COUNT(*) FROM dbo.Votes
   UNION ALL SELECT 'Badges', COUNT(*) FROM dbo.Badges
   UNION ALL SELECT 'PostHistory', COUNT(*) FROM dbo.PostHistory
   UNION ALL SELECT 'PostLinks', COUNT(*) FROM dbo.PostLinks
   UNION ALL SELECT 'Tags', COUNT(*) FROM dbo.Tags
   UNION ALL SELECT 'VoteTypes', COUNT(*) FROM dbo.VoteTypes
   ORDER BY TableName;"
```

### Performance Metrics

Expected replication performance (StackOverflow2010):
- Total dataset: ~10GB (compressed) / ~8.4GB (uncompressed .mdf)
- Estimated runtime: 10-30 minutes (depending on hardware)
- Memory usage: <128MB per task (spills to disk above threshold)
- Largest tables: Votes (~4.3M rows), PostHistory (~2.8M rows), Posts (~1.7M rows)

## License and Attribution

The StackOverflow2010 database is provided under **CC-BY-SA 3.0** license:
- Source: https://archive.org/details/stackexchange
- Compiled by: Brent Ozar Unlimited (https://www.brentozar.com)
- You are free to share and adapt this database, even commercially
- Attribution required to Stack Exchange Inc. and original authors
